{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kur1nbDyfIcb",
        "outputId": "f89f6553-0caa-4328-9ca0-390322e982b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/981.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m614.4/981.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=66a207a925a9e0b4ac6ed8751ab3fc50c9b015c23e9fc8b56e1c8d649cba1fa9\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "! pip install langdetect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyLsDWvne9WN",
        "outputId": "f6f3fedc-dadb-40aa-ea27-2a2f869f332e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from gensim.models import LdaModel\n",
        "from langdetect import detect\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Cvniqb2Vfh6G"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('all_ECB_speeches.csv',sep = '|')\n",
        "# Remove non-english speeches\n",
        "data_drop_nan = data.dropna(subset=data.columns)\n",
        "data.dropna(subset=data.columns, inplace=True)\n",
        "\n",
        "# Function to detect language of a given text\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "    except:\n",
        "        lang = \"Unknown\"\n",
        "    return lang\n",
        "\n",
        "# Apply the language detection function to each text in your DataFrame\n",
        "data['language'] = data['contents'].apply(detect_language)\n",
        "\n",
        "# Filter out the texts that are not in English\n",
        "data = data[data['language'] == 'en']\n",
        "# Drop null values\n",
        "data = data.dropna(subset=['speakers','contents'])\n",
        "# Reset index\n",
        "data = data.reset_index(drop=True)\n",
        "# Remove introduction\n",
        "for row in range (data.shape[0]):\n",
        "  try:\n",
        "    speech = re.split(\n",
        "        \" \\d+ (January|February|March|April|May|June|July|August|September|October|November|December) \\d{4} \",\n",
        "        data.loc[row, \"contents\"])\n",
        "    data.loc[row, \"contents\"] = speech[-1]\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "plYAsb-Wf1m7"
      },
      "outputs": [],
      "source": [
        "corpus = data['contents']\n",
        "\n",
        "# remove stopwords, punctuation, and normalize the corpus\n",
        "stop = set(stopwords.words('english'))\n",
        "exclude = set(string.punctuation)\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "def clean(doc):\n",
        "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
        "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
        "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
        "    return normalized\n",
        "\n",
        "clean_corpus = [clean(doc).split() for doc in corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YI_LzQE0f_FU"
      },
      "outputs": [],
      "source": [
        "# Creating document-term matrix\n",
        "dictionary = corpora.Dictionary(clean_corpus)\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jlBD-C6gDJh",
        "outputId": "d3d175f8-a413-473e-bc49-207800edd124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, '0.324*\"policy\" + 0.281*\"inflation\" + 0.225*\"monetary\"'), (1, '-0.430*\"inflation\" + 0.363*\"digital\" + 0.285*\"payment\"'), (2, '-0.357*\"digital\" + 0.336*\"bank\" + -0.332*\"inflation\"'), (3, '0.337*\"rate\" + -0.310*\"risk\" + -0.310*\"climate\"')]\n"
          ]
        }
      ],
      "source": [
        "# LSA model\n",
        "lsa_model = LsiModel(doc_term_matrix, num_topics=4, id2word = dictionary)\n",
        "\n",
        "# LSA model\n",
        "print(lsa_model.print_topics(num_topics=4, num_words=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-wm8K4AhZWy",
        "outputId": "346e9217-9d91-49b0-981f-83aabc1cedc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, '0.011*\"policy\" + 0.008*\"euro\" + 0.008*\"monetary\"'), (1, '0.010*\"inflation\" + 0.010*\"policy\" + 0.008*\"euro\"'), (2, '0.012*\"policy\" + 0.009*\"inflation\" + 0.009*\"euro\"'), (3, '0.011*\"bank\" + 0.011*\"policy\" + 0.009*\"inflation\"')]\n"
          ]
        }
      ],
      "source": [
        "# LDA model\n",
        "lda_model = LdaModel(doc_term_matrix, num_topics=4, id2word = dictionary)\n",
        "\n",
        "# Results\n",
        "print(lda_model.print_topics(num_topics=4, num_words=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Vt-ppm_HmA4c"
      },
      "outputs": [],
      "source": [
        "topic_categories_lsa = {\n",
        "    0: \"monetary policy\",\n",
        "    1: \"inflation\",\n",
        "    2: \"activity\",\n",
        "    3: \"economic crisis\"\n",
        "}\n",
        "topic_categories_lda = {\n",
        "    0: \"inflation\",\n",
        "    1: \"monetary policy\",\n",
        "    2: \"activity\",\n",
        "    3: \"economic crisis\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEe73kU3kFDm",
        "outputId": "d40a4878-5aa5-4c9a-ac28-ec17cdb0f3c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                            contents dominant_topic_lda\n",
            "0  As we approach the end of this legislative ter...    monetary policy\n",
            "1  More than 30 years after its inception, Econom...    monetary policy\n",
            "2    Today’s hearing is our last before the end o...           activity\n",
            "3  on the digital euro (CON/2023/34)”.     See Ar...           activity\n",
            "4  Over the past few years the euro area economy ...           activity\n",
            "                                            contents dominant_topic_lsa\n",
            "0  As we approach the end of this legislative ter...    monetary policy\n",
            "1  More than 30 years after its inception, Econom...    monetary policy\n",
            "2    Today’s hearing is our last before the end o...    monetary policy\n",
            "3  on the digital euro (CON/2023/34)”.     See Ar...          inflation\n",
            "4  Over the past few years the euro area economy ...    monetary policy\n"
          ]
        }
      ],
      "source": [
        "# Função para pré-processar o texto\n",
        "def preprocess(text):\n",
        "    return [lemma.lemmatize(word) for word in text.lower().split() if word not in stop and word not in exclude]\n",
        "\n",
        "# Convertendo texto para vetor usando o dicionário\n",
        "data['bow'] = data['contents'].apply(lambda x: dictionary.doc2bow(preprocess(x)))\n",
        "\n",
        "# Aplicando o modelo LSA para obter os tópicos\n",
        "data['lsa_topics'] = data['bow'].apply(lambda x: lsa_model[x])\n",
        "data['lda_topics'] = data['bow'].apply(lambda x: lda_model[x])\n",
        "\n",
        "# Extraindo o tópico dominante para cada documento\n",
        "data['dominant_topic_lsa'] = data['lsa_topics'].apply(lambda x: max(x, key=lambda item: abs(item[1]))[0] if x else None)\n",
        "data['dominant_topic_lda'] = data['lda_topics'].apply(lambda x: max(x, key=lambda item: abs(item[1]))[0] if x else None)\n",
        "\n",
        "# Substituindo números dos tópicos por descrições textuais\n",
        "data['dominant_topic_lsa'] = data['dominant_topic_lsa'].map(topic_categories_lsa)\n",
        "data['dominant_topic_lda'] = data['dominant_topic_lda'].map(topic_categories_lda)\n",
        "\n",
        "# Removendo coluna intermediária 'bow' se desejado\n",
        "data.drop('bow', axis=1, inplace=True)\n",
        "\n",
        "# Exibindo o DataFrame atualizado\n",
        "print(data[['contents', 'dominant_topic_lda']][0:5])\n",
        "print(data[['contents', 'dominant_topic_lsa']][0:5])\n",
        "\n",
        "data.to_csv(\"speeches_topics.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
